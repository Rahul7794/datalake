### Standard connector configuration
## Fill in your values in these:
## These must have exactly these values:
# The Java class for the connector
name=S3SourceConnectorParquet
connector.class=io.confluent.connect.s3.S3SinkConnector
# The key converter for this connector
key.converter=org.apache.kafka.connect.storage.StringConverter
# The value converter for this connector
value.converter=org.apache.kafka.connect.json.JsonConverter
# Identify, if value contains a schema.
# Required value converter is `org.apache.kafka.connect.json.JsonConverter`.
value.converter.schemas.enable=false
# The type of data format used to write data to the GCS output files.
# The supported values are: `csv`, `json`, `jsonl` and `parquet`.
# Optional, the default is `csv`.
format.output.type=jsonl
# A comma-separated list of topics to use as input for this connector
# Also a regular expression version `topics.regex` is supported.
# See https://kafka.apache.org/documentation/#connect_configuring
topics=test
### Connector-specific configuration
### Fill in you values
# AWS Access Key ID
aws.access.key.id=test
# AWS Access Secret Key
aws.secret.access.key=test
#AWS Region
s3.region=us-east-1
#File name template
file.name.template=dir1/dir2/{{topic}}-{{partition:padding=true}}-{{start_offset:padding=true}}.gz
#The name of the S3 bucket to use
#Required.
s3.bucket.name=test
store.url=http://localstack:4566
# The set of the fields that are to be output, comma separated.
# Supported values are: `key`, `value`, `offset`, `timestamp` and `headers`.
# Optional, the default is `value`.
format.output.fields=key,value,offset,timestamp
# The option to enable/disable wrapping of plain values into additional JSON object(aka envelope)
# Optional, the default value is `true`.
format.output.envelope=true
# The compression type used for files put on GCS.
# The supported values are: `gzip`, `snappy`, `zstd`, `none`.
# Optional, the default is `none`.
file.compression.type=gzip
partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
path.format='year'=YYYY/'month'=MM/'day'=dd
timestamp.extractor=Record
storage.class=io.confluent.connect.s3.storage.S3Storage
timezone=UTC
locale=en